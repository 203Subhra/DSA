{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg4BKGJBCM1Hk5KQXRcsPk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neon12-ofx/DSA/blob/main/IntroNlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION**"
      ],
      "metadata": {
        "id": "gjuTHl389QCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHqV1TF1ko4s",
        "outputId": "38ff1f25-b733-49ed-8136-6b92e4217d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "s = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "# Convert to lowercase and tokenize\n",
        "tokens = word_tokenize(s.lower())\n",
        "\n",
        "# Get English stopwords\n",
        "sw = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Filter stopwords\n",
        "filtered_sentence = [word for word in tokens if word not in sw]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(s)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS7xgvtzcKY2",
        "outputId": "bb1e54de-e94b-4443-922d-ded7a627c0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample sentence, showing off the stop words filtration.\n",
            "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEMMING**"
      ],
      "metadata": {
        "id": "tJTokY319ZYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. POSTERSTEMMER\n",
        "\n"
      ],
      "metadata": {
        "id": "crw_WsBy9gbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "#creating instances of Poster Stremmer\n",
        "ps=PorterStemmer()\n",
        "\n",
        "words=[\"running\",\"jumps\",\"happily\",\"running\"]\n",
        "\n",
        "for i in words:\n",
        "    stemmed_word=ps.stem(i)\n",
        "    print(stemmed_word)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noxuVgsb4xXh",
        "outputId": "3c20610f-9f1a-4a04-f31e-60a504303653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "jump\n",
            "happili\n",
            "run\n",
            "['running', 'jumps', 'happily', 'running']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.  SWONBALL STEMMER\n",
        "\n"
      ],
      "metadata": {
        "id": "n69xuh5u9rkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sl=SnowballStemmer(\"english\")\n",
        "\n",
        "words=[\"running\",\"jumped\",\"happily\"]\n",
        "\n",
        "for i in words:\n",
        "  stemmed_w=[sl.stem(i)]\n",
        "  print(stemmed_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqPCq3N599rT",
        "outputId": "2fe9a189-a859-40b9-fa83-e65b95dd5033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run']\n",
            "['jump']\n",
            "['happili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " 3.  Regexp Stemmering\n",
        "\n"
      ],
      "metadata": {
        "id": "ZvwBgs8Lnoi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "custom_r=r'ing$'\n",
        "\n",
        "reg_st=RegexpStemmer(custom_r)\n",
        "\n",
        "words=[\"running\",\"jumping\",\"happily\"]\n",
        "\n",
        "for i in words:\n",
        "  stemmed_w=[reg_st.stem(i)]\n",
        "  print(stemmed_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGgJtnexnvKK",
        "outputId": "a53aaa41-49e1-49b5-8d58-b926cfc2dd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['runn']\n",
            "['jump']\n",
            "['happily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "olw2Z5ukon6O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAetVZBRou7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}